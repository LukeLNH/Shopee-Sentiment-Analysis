{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Code from this notebook is referenced from https://www.kaggle.com/sanxuwen/shopee-sentiment-analysis-2nd-place-solution\n\n#training sample and paramaters in this notebook are much smaller than in an actual competition to save on training time. In addition,\n#if an actual submission is desired, the classification results should be averaged out between most/all the models listed.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom transformers import TFAutoModel ,TFAutoModelForSequenceClassification, AutoTokenizer, TFBertModel\nfrom sklearn.model_selection import train_test_split\nimport os\nimport emoji\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/shopee-code-league-20/_DS_Sentiment_Analysis/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data.review\ny = train_data.rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessor\n#Data cleaning functions emoji_cleaning, review_cleaning, and encode function from \n    #https://www.kaggle.com/sanxuwen/shopee-sentiment-analysis-2nd-place-solution\n\n#I have decided not to one hot encode the targets\nclass Sentence_Preprocessor():\n    def __init__(self, tokenizer, MAX_LEN = 300):\n        self._tokenizer = tokenizer\n        self._MAX_LEN = MAX_LEN\n    \n    \n    #Cleaning out all emojis\n    def emoji_cleaning(self, text):\n\n        # Change emoji to text\n        text = emoji.demojize(text).replace(\":\", \" \")\n\n        # Delete repeated emoji\n        tokenizer = text.split()\n        repeated_list = []\n\n        for word in tokenizer:\n            if word not in repeated_list:\n                repeated_list.append(word)\n\n        text = ' '.join(text for text in repeated_list)\n        text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n        return text\n    \n    #Shortening repeated words or words with drawn out letters like \"shoooooppppppeeeeeeeeeee\"\n    def review_cleaning(self, text):\n\n        # delete lowercase and newline\n        text = text.lower()\n        text = re.sub(r'\\n', '', text)\n        text = re.sub('([.,!?()])', r' \\1 ', text)\n        text = re.sub('\\s{2,}', ' ', text)\n\n        # change emoticon to text\n        text = re.sub(r':\\(', 'dislike', text)\n        text = re.sub(r': \\(\\(', 'dislike', text)\n        text = re.sub(r':, \\(', 'dislike', text)\n        text = re.sub(r':\\)', 'smile', text)\n        text = re.sub(r';\\)', 'smile', text)\n        text = re.sub(r':\\)\\)\\)', 'smile', text)\n        text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n        text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n\n\n        tokenizer = text.split()\n\n        return ' '.join([text for text in tokenizer])\n    \n    def clean_sentence(self, sentence):\n        \n        sentence = self.emoji_cleaning(sentence)\n        \n        sentence = self.review_cleaning(sentence)\n        \n        return sentence\n\n    \n    def preprocess_sentence(self, sentence):\n        \n        sentence = self.clean_sentence(sentence)\n        \n        sentence = self._tokenizer.encode(sentence, pad_to_max_length = True, max_length = self._MAX_LEN)\n        \n        return sentence\n    \n    def one_hot_encode(self, y):\n        \n        y = np.array(y)\n        \n        y_post = np.zeros(y.shape[0] * 5).reshape(y.shape[0], 5)\n        \n        for index in range(y.shape[0]):\n            y_post[index, y[index]-1] = 1\n            \n        return y_post\n    \n    def preprocess(self, X, y = None):\n        \n        #This function assumes X and y are pd Series, since thats how the data will be read\n        \n        X = X.apply(self.clean_sentence)\n        \n        X = self._tokenizer.batch_encode_plus(\n                 X, \n                 return_attention_masks=True, \n                 return_token_type_ids=False,\n                 pad_to_max_length=True,\n                 max_length=PADDED_LEN)\n        \n        if y is not None:\n            return np.array(X['input_ids']), self.one_hot_encode(y)\n        else:\n            return np.array(X['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipeline\nclass Pipeline():\n    \n    def __init__(self, tokenizer, model, max_len = PADDED_LEN):\n        \n        self._model = model\n        self._preprocessor = Sentence_Preprocessor(tokenizer, max_len)\n        \n        \n    def preprocess(self, X_train, X_test, y_train, y_test):\n        \n        print('preprocessing...')\n        \n        X_train, y_train = self._preprocessor.preprocess(X_train, y_train)\n        \n        X_test, y_test = self._preprocessor.preprocess(X_test, y_test)\n        \n        print('finished preprocessing!')\n        \n        return X_train, X_test, y_train, y_test\n    \n    \n    def to_tensorflow_dataset(self, X_train, X_test, y_train, y_test, batch_size):\n        \n        print('making Tensorflow Dataset...')\n        \n        train_dataset = (tf.data.Dataset\n                        .from_tensor_slices((X_train, y_train))\n                        .repeat()\n                        .shuffle(20)\n                        .batch(BATCH_SIZE))\n        \n        test_dataset = (tf.data.Dataset\n                        .from_tensor_slices((X_test, y_test))\n                        .shuffle(20)\n                        .batch(BATCH_SIZE))\n        \n        print('completed')\n        \n        return train_dataset, test_dataset\n        \n    def fit(self, X_train, X_test, y_train, y_test, epochs = 2, batch_size = 100, auto_preprocess = True):\n        \n        if auto_preprocess:\n            X_train, X_test, y_train, y_test = self.preprocess(X_train, X_test, y_train, y_test)\n        \n        train_dataset, test_dataset = self.to_tensorflow_dataset(X_train, X_test, y_train, y_test, batch_size)\n\n        print('training...')\n        \n        self._model.fit(\n            train_dataset,\n            steps_per_epoch = X_train.shape[0],\n            validation_data = test_dataset,\n            epochs = epochs)\n        \n        \n    def predict_single_review(self, X_pred, final_category_only = False):\n        \n        X_pred = np.array(self._preprocessor.preprocess_sentence(X_pred))\n        X_pred = X_pred.reshape((1, X_pred.shape[0]))\n        \n        if final_category_only:\n            return np.argmax(model.predict(X_pred), axis = 1) + 1 #add 1 because ratings are 1-5, not 0-4\n        return model.predict(X_pred)\n    \n    \n    def predict_many_reviews(self, X_pred, final_category_only = False):\n        \n        X_pred = self._preprocessor.preprocess(X_pred)\n        \n        X_pred = (tf.data.Dataset\n                  .from_tensor_slices(X_pred)\n                  .batch(20))\n        \n        if final_category_only:\n            return np.argmax(model.predict(X_pred), axis = 1) + 1\n        return model.predict(X_pred)\n    \n    \n    def get_preprocessor(self):\n        return self._preprocessor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# emoji_index = train_data.apply(lambda seq : any(char in emoji.UNICODE_EMOJI for char in seq))\n# train_data[emoji_index] = train_data[emoji_index].apply(emoji_cleaning)\n# train_data = train_data.apply(review_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bert model from https://huggingface.co/transformers/model_doc/auto.html\n\nbase_model_names = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base', 'xlm-mlm-en-2048'] #'xlnet-base-cased'\n\n# base_model_name = base_model_names[np.random.randint(len(base_model_names))]\n\nbase_model_name = 'bert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nbase_model = TFAutoModelForSequenceClassification.from_pretrained(base_model_name)\n\nbase_model.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize the average length of a review after cleaning\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#the first 1000 reviews were used to save time\nreviews = train_data['review'][:1000]\n\n#create a quick preprocessor to clean the data\npreprocessor = Sentence_Preprocessor(tokenizer)\n\nreviews = reviews.apply(preprocessor.clean_sentence)\n\nsns.distplot(reviews.str.len())\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Padding reviews to a length of 300 should be safe\n#Defining Hyper Parameters here\nPADDED_LEN = 5 #300\nEPOCHS = 5\nBATCH_SIZE = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code from https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\ndef build_model(base_model, num_labels):\n    \n    inputs = tf.keras.layers.Input(shape = (PADDED_LEN,), dtype=tf.int32)\n    \n# TODO in the future: add attention masks\n    \n    embeddings = base_model(inputs)[0] #[:,0,:] \n    #the results of the model are wrapped in a tuple (), so we index [0] to extract the results\n    \n    out = tf.keras.layers.Reshape((1,num_labels))(embeddings)\n    \n    out = tf.keras.layers.Dropout(0.2)(out)\n    \n    out = tf.keras.layers.Conv1D(num_labels * 8, 1, padding = \"causal\", activation = 'relu')(out)\n    \n    out = tf.keras.layers.Conv1D(num_labels * 4, num_labels, padding = \"causal\", activation = 'relu')(out)\n    \n    out = tf.keras.layers.Conv1D(num_labels, num_labels, padding = \"causal\", activation = 'relu')(out)\n    \n    out = tf.keras.layers.GlobalAveragePooling1D()(out)\n    \n    added = tf.keras.layers.Add()([embeddings, out]) #Residual connection\n    \n    out = tf.keras.layers.Dense(5, activation = 'softmax')(added)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = out)\n    \n    model.compile(optimizer = \"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(base_model, base_model.config.num_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(tokenizer, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#When working with tensorflor Datasets, using np is much easier than pd. make sure X is a 2D array, and y is a 1D array\n# train_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((X_train, y_train))\n#     .repeat()\n#     .shuffle(1024)\n#     .batch(20)\n# )\n\n# !!! Note about this model and tensorflow Datasets:\n# A single sentence currently has a shape of (PADDED_LEN,). Thus, when trying to fit/predict for a single sentence, the model reads the\n    #shape of the sentence, sees shape[0] is PADDED_LEN, and assumes that there are PADDED_LEN sentences instead of just 1 sentence. \n    \n    #Tensorflow Dataset batch method solves this problem since when batching, a batch dimension is added to the shape\n    #of the sample: (BATCH_SIZE, _shape_). Hence, when we pass the tensorflow batches to the model, the model reads \n    #that there are BATCH_SIZE number of sentences (which is correct), and outputs the right number of predictions.\n    \n    #An alternative for using tensorflow dataset batch to predict a sentence is to reshape the sentence into \n    #shape (1, PADDED_LEN). That way, the model will see shape[0] is 1 and correctly think that there is only 1 sentence\n    \n    #In light of this, it is clear that for training/predictin large datasets, we should use TFDS batch, while if only predicting for a\n    #single sentence, reshaping is more efficient\n    \n#TLDR: Problem: when predicting for a single sentence, model outputs PADDED_LEN different predictions\n    #Reason: a single sentence has shape (PADDED_LEN,). Model misinterprets the input as having PADDED_LEN sentences instead.\n    #Solution: reshape the sentence into shape (1,PADDED_LEN) or use Tensorflow Datasets batch method to batch many sentences together.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.predict_many_reviews(train_data.review[:13], final_category_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}